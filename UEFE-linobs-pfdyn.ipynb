{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "195b2d98",
   "metadata": {},
   "source": [
    "## Expected Free Energy for oscillator with nonlinear observations\n",
    "\n",
    "Wouter Kouw, last update: 20-11-2022\n",
    "\n",
    "### System\n",
    "\n",
    "Consider a [harmonic oscillator](https://en.wikipedia.org/wiki/Harmonic_oscillator#Driven_harmonic_oscillators) with displacement $x(t)$, driving force $u(t)$ and noise $w(t)$. The continuous-time dynamics of the system are:\n",
    "\n",
    "$$\\begin{align*}\n",
    "m \\frac{d^2 x(t)}{dt^2} =&\\ - c \\frac{d x(t)}{dt} - k x(t) + u(t) + w(t)\\, ,\n",
    "\\end{align*}$$\n",
    "where \n",
    "$$\n",
    "m     = \\text{mass} \\, , \\quad\n",
    "c     = \\text{damping} \\, , \\quad\n",
    "k     = \\text{spring stiffness} \n",
    "$$\n",
    "constitute the physical parameters. \n",
    "\n",
    "#### Multivariate first-order system\n",
    "\n",
    "We will first adopt a more concise notation and then divide by the leading coefficient:\n",
    "\n",
    "$$ x'' = \\frac{-c}{m} x' + \\frac{-k}{m} x + \\frac{1}{m} u + \\frac{1}{m} w \\, .$$\n",
    "\n",
    "With the variable substitution $z = [x \\ x']$, I cast the above system into a multi-variate first-order form:\n",
    "\n",
    "$$ \\begin{bmatrix} x' \\\\ x'' \\end{bmatrix} = \\begin{bmatrix} 0 & 1 \\\\ \\frac{-k}{m} & \\frac{-c}{m} \\end{bmatrix} \\begin{bmatrix} x \\\\ x' \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ \\frac{1}{m} \\end{bmatrix} u + \\begin{bmatrix} 0 \\\\ \\frac{1}{m} \\end{bmatrix} w\\, .$$\n",
    "\n",
    "#### Discretization\n",
    "\n",
    "For the Van Loan discretization of the Wiener process, I first consider a first-order approximation of the matrix exponential of the transition matrix: \n",
    "\n",
    "$$F(t) = I + \\begin{bmatrix} 0 & 1 \\\\ \\frac{-k}{m} & \\frac{-c}{m} \\end{bmatrix} t \\, .$$\n",
    "\n",
    "Then, I will consider the continuous-time covariance matrix of the noise in multi-variate form:\n",
    "\n",
    "$$\\mathbb{V}[\\begin{bmatrix} 0 \\\\ \\frac{1}{m} \\end{bmatrix} w] = \\begin{bmatrix} 0 \\\\ \\frac{1}{m} \\end{bmatrix} \\mathbb{V}[w] \\begin{bmatrix} 0 \\\\ \\frac{1}{m} \\end{bmatrix}^{\\top} = \\begin{bmatrix} 0 & 0 \\\\ 0 & \\frac{\\sigma^2}{m^2} \\end{bmatrix} \\triangleq V \\, .$$\n",
    "\n",
    "Now, I can integrate the evolution of the covariance matrix from over the duration of a time-step. If all time-steps are equally long, e.g., of size $\\Delta t$, then\n",
    "\n",
    "$$Q = \\int_0^{\\Delta t} F(t) V F(t)^{\\top} dt = \\frac{\\sigma^2}{m^2} \\begin{bmatrix} \\frac{\\Delta t^3}{3} & \\frac{-c}{m}\\frac{\\Delta t^3}{3} + \\frac{\\Delta t^2}{2} \\\\ \\frac{-c}{m}\\frac{\\Delta t^3}{3} + \\frac{\\Delta t^2}{2} & \\frac{c^2}{m^2}\\frac{\\Delta t^3}{3} + \\frac{\\Delta t^2}{2} + \\Delta t \\end{bmatrix} \\, ,$$\n",
    "\n",
    "where $\\sigma^2 = \\mathbb{V}[w]$. A forward Euler discretization for the state transition yields:\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\left( \\begin{bmatrix} x_{k+1} \\\\ x'_{k+1} \\end{bmatrix} - \\begin{bmatrix} x_{k} \\\\ x'_{k} \\end{bmatrix} \\right) / \\Delta t =&\\ \\begin{bmatrix} 0 & 1 \\\\ \\frac{-k}{m} & \\frac{-c}{m} \\end{bmatrix} \\begin{bmatrix} x_k \\\\ x'_k \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ \\frac{1}{m} \\end{bmatrix} u_k \\, ,\n",
    "\\end{align*}$$\n",
    "\n",
    "So, we end up with:\n",
    "\n",
    "$$\\begin{align*}\n",
    "z_k =&\\ \\underbrace{\\begin{bmatrix} 1 & \\Delta t \\\\ \\frac{-k}{m} \\Delta t & \\frac{-c}{m} \\Delta t + 1 \\end{bmatrix}}_{A} z_{k-1} + \\underbrace{\\begin{bmatrix} 0 \\\\ \\frac{1}{m} \\Delta t \\end{bmatrix}}_{B} u_k + q_k \\quad \\text{where} \\quad q_k \\sim \\mathcal{N}(0,Q) \\, .\n",
    "\\end{align*}$$\n",
    "\n",
    "We have noisy observations of a nonlinear function $g$ of the displacement, \n",
    "\n",
    "$$ y_k = g(z_k) + r_k \\quad \\text{where} \\quad r_k \\sim \\mathcal{N}(0, R) \\, .$$\n",
    "\n",
    "This gives an overall probabibilistic state-space model of the form:\n",
    "\n",
    "$$\\begin{align}\n",
    "p(z_0) &= \\mathcal{N}(z_0 \\mid m_0, S_0) \\\\\n",
    "p(z_k \\mid z_{k-1}, u_k) &= \\mathcal{N}(z_k \\mid Az_{k-1} + Bu_k, Q) \\\\\n",
    "p(y_k \\mid z_k) &= \\mathcal{N}(y_k \\mid g(z_k), R) \\, .\n",
    "\\end{align}$$\n",
    "\n",
    "We can simulate the behaviour of this system as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f29cf9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Pkg\n",
    "# Pkg.activate(\".\")\n",
    "# Pkg.instantiate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "336ca769",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      "  [1] _include_from_serialized(pkg::Base.PkgId, path::String, depmods::Vector{Any})",
      "    @ Base ./loading.jl:807",
      "  [2] _tryrequire_from_serialized(modkey::Base.PkgId, path::String, sourcepath::String, depmods::Vector{Any})",
      "    @ Base ./loading.jl:938",
      "  [3] _require_search_from_serialized(pkg::Base.PkgId, sourcepath::String, build_id::UInt64)",
      "    @ Base ./loading.jl:1028",
      "  [4] _require(pkg::Base.PkgId)",
      "    @ Base ./loading.jl:1315",
      "  [5] _require_prelocked(uuidkey::Base.PkgId)",
      "    @ Base ./loading.jl:1200",
      "  [6] macro expansion",
      "    @ ./loading.jl:1180 [inlined]",
      "  [7] macro expansion",
      "    @ ./lock.jl:223 [inlined]",
      "  [8] require(into::Module, mod::Symbol)",
      "    @ Base ./loading.jl:1144",
      "  [9] eval",
      "    @ ./boot.jl:368 [inlined]",
      " [10] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base ./loading.jl:1428"
     ]
    }
   ],
   "source": [
    "using Revise\n",
    "using Optim\n",
    "using ForwardDiff\n",
    "using ProgressMeter\n",
    "using LinearAlgebra\n",
    "using ControlSystems\n",
    "using Distributions\n",
    "using Plots\n",
    "default(label=\"\", grid=false, linewidth=3, markersize=3, margin=10Plots.pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc0ceae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System parameters\n",
    "mass = 3.0\n",
    "friction = 0.9\n",
    "stiffness = 2.0\n",
    "\n",
    "# Substituted parameters\n",
    "θ1_true = -stiffness/mass\n",
    "θ2_true = -friction/mass\n",
    "θ3_true = 1/mass\n",
    "θ_true = [θ1_true, θ2_true, θ3_true]\n",
    "\n",
    "# Temporal variables\n",
    "Δt = 0.1\n",
    "len_time = 200\n",
    "time = range(0, step=Δt, length=len_time)\n",
    "\n",
    "# Matrices\n",
    "A = [1 Δt; θ1_true*Δt θ2_true*Δt+1]\n",
    "B = [0, θ3_true*Δt]\n",
    "C = [1, 0]\n",
    "\n",
    "# Inverse functions for parameters\n",
    "g1(y,x,u, θ2, θ3) = (y[2] - (Δt*θ2+1)*x[2] - Δt*θ3*u)/(Δt*x[1])\n",
    "g2(y,x,u, θ1, θ3) = (y[2] - Δt*θ1*x[1] - Δt*θ3*u)/(Δt*x[2]) - 1/Δt\n",
    "g3(y,x,u, θ1, θ2) = (y[2] - Δt*θ1*x[1] - (Δt*θ2 + 1)*x[2])/(Δt*u)\n",
    "    \n",
    "# Measurement noise\n",
    "R = 1e-3;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cc39986",
   "metadata": {},
   "outputs": [],
   "source": [
    "function update(z_kmin1, u_k)\n",
    "   \"Update environment\" \n",
    "    \n",
    "    # State transition\n",
    "    z_k = A*z_kmin1 + B*u_k\n",
    "    \n",
    "    # Emit noisy observation\n",
    "    y_k = rand(Normal(dot(C,z_k), sqrt(R)))\n",
    "    \n",
    "    return y_k, z_k\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450a6ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control\n",
    "u = [sin.(time[1:Int64(round(len_time/4))] .* 2/π); \n",
    "     zeros(len_time-Int64(round(len_time/4)))]\n",
    "\n",
    "# Setpoint (desired observation)\n",
    "z_star = [-0.8, 0.0]\n",
    "y_star = dot(C,z_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8389f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial state\n",
    "z_0 = [0.0, 0.0]\n",
    "\n",
    "# Preallocate\n",
    "z = zeros(2,len_time)\n",
    "y = zeros(len_time)\n",
    "\n",
    "# Start recursion\n",
    "z_kmin1 = z_0\n",
    "for k in 1:len_time\n",
    "    \n",
    "    # Update system\n",
    "    y[k], z[:,k] = update(z_kmin1, u[k])\n",
    "    \n",
    "    # Update recursion\n",
    "    z_kmin1 = z[:,k]\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127f956a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = plot(time, z[1,:], xlabel=\"time (s)\", label=\"displacement\")\n",
    "scatter!(time, y, color=\"black\", label=\"observations\")\n",
    "plot!(time, y_star*ones(len_time), color=\"green\", label=\"setpoint\")\n",
    "p2 = plot(time, z[2,:], xlabel=\"time (s)\", label=\"velocity\")\n",
    "p3 = plot(time, u, color=\"red\", xlabel=\"time (s)\", label=\"control\")\n",
    "plot(p1, p2, p3, layout=(3,1), size=(900,800))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe99f22e",
   "metadata": {},
   "source": [
    "## Expected Free Energy\n",
    "\n",
    "From the agent's viewpoint, the evolution of these future variables are constrained by its generative model, rolled out into the future:\n",
    "$$\\begin{aligned}\n",
    "p(y,z,u) &= \\underbrace{q(z_{k})}_{\\substack{\\text{current}\\\\ \\text{state}}} \\cdot \\underbrace{\\prod_{t=k+1}^{k+T} p(y_t|z_t) \\cdot p(z_t | z_{t-1}, u_t) p(u_t)}_{\\text{generative model roll-out to future}} \\, ,\n",
    "\\end{aligned}$$\n",
    "\n",
    "where $y = (y_t, \\dots, y_T)$, $z = (z_{t-1}, \\dots, z_T)$ and $u = (u_t, \\dots, u_T)$. \n",
    "\n",
    "Consider the varational free energy functional for estimating posterior beliefs $q(z,u)$ over future states and control signals, with an expectation over future observations $q(y \\mid z)$: \n",
    "\n",
    "$$\\begin{aligned}\n",
    "H[q] &= \\overbrace{\\iint q(y \\mid z)}^{\\text{marginalize }y} \\bigg( \\overbrace{\\int q(z,u) \\log \\frac{q(z,u)}{p(y,z,u)} }^{\\text{variational free energy}}\\bigg) \\mathrm{d}u \\, \\mathrm{d}y \\, \\mathrm{d}z \\\\\n",
    "&= \\int q(y,z,u) \\log \\frac{q(z,u)}{p(y,z,u)} \\mathrm{d}u \\, \\mathrm{d}y \\, \\mathrm{d}z \\\\\n",
    "&= \\int q(y,z|u) q(u) \\log \\frac{q(z|u) q(u)}{p(y,z|u) p(u)} \\mathrm{d}u \\, \\mathrm{d}y \\, \\mathrm{d}z \\\\\n",
    "&= \\int q(u) \\bigg(\\sum_{y,z} q(y,z|u) \\log \\frac{q(s|u) q(u)}{p(y,z|u) p(u)}\\bigg) \\mathrm{d}u \\, \\mathrm{d}y \\, \\mathrm{d}z \\\\\n",
    "&= \\int q(u) \\bigg( \\log q(u) + \\log \\frac{1}{p(u)}+ \\underbrace{q(y,z|u) \\log \\frac{q(z|u)}{p(y,z|u)}}_{G(u)}\\bigg) \\mathrm{d}u \\, \\mathrm{d}y \\, \\mathrm{d}z \\\\\n",
    "&= \\int q(u) \\log \\frac{q(u)}{p(u)\\exp\\left(- G(u)\\right) } \\mathrm{d}u  \\, .\n",
    "\\end{aligned}$$\n",
    "\n",
    "We can recognize a KL-divergence between $q(u)$ and the function $p(u)\\exp\\left(- G(u)\\right)$, which will be minimal when $q(u) = p(u)\\exp\\left(- G(u)\\right)$. The $G$ function is known as the _Expected Free Energy_. Let's look at it in detail for a future observation $t$,\n",
    "\n",
    "$$ G(u_t) = \\iint q(y_t, z_t \\mid u_t) \\log \\frac{q(z_t \\mid u_t)}{p(y_t, z_t \\mid u_t)} \\mathrm{d}y_t \\, \\mathrm{d}z_t \\, ,$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\begin{aligned}\n",
    "% q(y_t \\mid z_t) &= \\mathcal{N}(y_t \\mid Cz_t, R) \\\\\n",
    "% q(z_t \\mid u_t) &= \\int p(z_t \\mid z_{t-1}, u_t) q(z_{t-1}) \\mathrm{d}z_{t-1} \\\\\n",
    "% &= \\mathcal{N}(z_t \\mid A m_{t-1} + Bu_t, AS_{t-1}A^{\\top} + Q) \\\\\n",
    "p(y_t, z_t \\mid u_t) &= \\int p(y_t \\mid z_t) p(z_t \\mid z_{t-1}, u_t) q(z_{t-1}) \\mathrm{d}z_{t-1} \\\\\n",
    "&= \\mathcal{N}(\\begin{bmatrix} y_t \\\\ z_t \\end{bmatrix} \\mid \\begin{bmatrix} C(A m_{t-1} + Bu_t) \\\\ A m_{t-1} + Bu_t \\end{bmatrix}, \\begin{bmatrix} C(AS_{t-1}A^{\\top} + Q) C^{\\top} + R & C(AS_{t-1}A^{\\top} + Q) \\\\ (AS_{t-1}A^{\\top} + Q)C^{\\top} & AS_{t-1}A^{\\top} + Q \\end{bmatrix}) \\, ,\n",
    "\\end{aligned}$$\n",
    "\n",
    "with the approximate posterior for the previous state $q(z_{t-1}) = \\mathcal{N}(z_{t-1} \\mid m_{t-1}, S_{t-1})$. The distribution $q(z_t \\mid u_t)$ is constructed from the state transition:\n",
    "\n",
    "$$q(z_t \\mid u_t) = \\int p(z_t \\mid z_{t-1}, u_t) q(z_{t-1}) \\mathrm{d}z_{t-1} = \\mathcal{N}(z_t \\mid A m_{t-1} + Bu_t, AS_{t-1}A^{\\top} + Q) \\, ,$$\n",
    "\n",
    "and $q(y_t, z_t \\mid u) = p(y_t \\mid z_t) q(z_t \\mid u_t)$ which is equivalent to $p(y_t, z_t \\mid u_t)$ in this case. Note that - during planning and in linear Gaussian models - we can drive the KL-divergence between the approximate posteriors $q$ and the true posteriors $p$ to 0, thus allowing us to swap $q$ for $p$.\n",
    "\n",
    "We will decompose this EFE function into ambiguity plus risk terms (see [lecture slides](https://nbviewer.org/github/bertdv/BMLIP/blob/master/lessons/notebooks/Intelligent-Agents-and-Active-Inference.ipynb) for more detail). To do so, we will decompose $p(y_t,z_t \\mid u_t)$ into $p^\\prime(y_t)p(z_t|y_t,u_t)$ where $p\\prime(y_t)$ is a distribution over the goal state, a.k.a. a _goal prior_; $p^\\prime(y) = \\mathcal{N}(y \\mid m_*, s_*)$. The derivation goes as follows:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "G(u_t) &= \\int  q(y_t,z_t|u_t) \\log \\frac{q(z_t|u_t)}{p^\\prime(y_t)p(z_t|y_t,u_t)} \\, \\mathrm{d}y_t \\, \\mathrm{d}z_t \\\\\n",
    "&= \\int  q(y_t, z_t \\mid u_t) \\log \\frac{q(z_t \\mid u_t)}{p^\\prime(y_t)} \\frac{1}{p(z_t \\mid y_t, u_t)} \\, \\mathrm{d}y_t \\, \\mathrm{d}z_t \\\\\n",
    "&= \\int  q(y_t, z_t \\mid u_t) \\log \\frac{q(z_t \\mid u_t)}{p^\\prime(y_t)} \\frac{p(y_t \\mid u_t)}{p(y_t \\mid z_t)p(z_t \\mid u_t)} \\, \\mathrm{d}y_t \\, \\mathrm{d}z_t \\\\\n",
    "&= \\int  q(y_t,z_t|u_t) \\log \\frac{q(z_t|u_t)}{p(y_t|z_t)p(z_t|u_t)} \\frac{p(y_t|u_t)}{p^\\prime(y_t)} \\, \\mathrm{d}y_t \\, \\mathrm{d}z_t \\\\\n",
    "&= \\int  q(y_t,z_t|u_t) \\log \\frac{q(z_t|u_t)}{p(y_t|z_t)p(z_t|u_t)} \\, \\mathrm{d}z_t \\, \\mathrm{d}y_t + \\int q(y_t,z_t|u_t) \\log \\frac{p(y_t|u_t)}{p^\\prime(y_t)} \\, \\mathrm{d}y_t \\, \\mathrm{d}z_t \\\\\n",
    "&= \\int  p(z_t|u_t) p(y_t|z_t) \\log \\frac{1}{p(y_t|z_t)} \\, \\mathrm{d}z_t \\, \\mathrm{d}y_t + \\int p(z|u) p(y|z) \\log \\frac{p(y_t|u_t)}{p^\\prime(y_t)} \\, \\mathrm{d}z_t \\, \\mathrm{d}y_t \\\\\n",
    "&= \\underbrace{\\int  p(z_t|u_t) \\int p(y_t|z_t) \\log \\frac{1}{p(y_t|z_t)} \\, \\mathrm{d}z_t \\, \\mathrm{d}y_t}_{\\text{ambiguity}} + \\underbrace{\\int p(y_t|u_t) \\log \\frac{p(y_t|u_t)}{p^\\prime(y_t)} \\, \\mathrm{d}y_t}_{\\text{risk}} \\ \\, ,\n",
    "\\end{aligned}$$\n",
    "\n",
    "Ambiguity is an expectation over an entropy, which - in the case of a Gaussian likelihood - is actually independent of the state,\n",
    "\n",
    "$$\\int p(y_t|z_t) \\log p(y_t|z_t) \\mathrm{d}y_t = \\frac{1}{2}\\big(\\log 2\\pi R - 1\\big) \\, ,$$\n",
    "\n",
    "so the expectation over $p(z_t|u_t)$ doesn't even apply. As you may have noticed, the control $u_t$ doesn't affect ambiguity which means there is no information-seeking behaviour in linear Gaussian state-space models.\n",
    "\n",
    "The risk term is a KL-divergence between the Gaussian,\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    p(y_t|u_t) &= \\int p(y_t, z_t \\mid u_t) \\, \\mathrm{d}z_t  \\\\\n",
    "    &= \\int \\mathcal{N}(\\begin{bmatrix} y_t \\\\ z_t \\end{bmatrix} \\mid \\begin{bmatrix} C(A m_{t-1} + Bu_t) \\\\ A m_{t-1} + Bu_t \\end{bmatrix}, \\begin{bmatrix} C(AS_{t-1}A^{\\top} + Q) C^{\\top} + R & C(AS_{t-1}A^{\\top} + Q) \\\\ (AS_{t-1}A^{\\top} + Q)C^{\\top} & AS_{t-1}A^{\\top} + Q \\end{bmatrix}) \\, \\mathrm{d}z_t \\\\\n",
    "    &= \\mathcal{N}(y_t \\mid C(A m_{t-1} + Bu_t) , C(AS_{t-1}A^{\\top} + Q) C^{\\top} + R) \\, ,\n",
    "    \\end{aligned}$$\n",
    "    \n",
    "and the goal prior $p^\\prime(y_t)$. The formula for the KL-divergence between two Gaussians is (see [wiki](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Multivariate_normal_distributions)):\n",
    "\n",
    "$$\\begin{aligned}\n",
    "D_{\\text{KL}}( \\mathcal{N}(x \\mid m_1, \\sigma_1) || \\mathcal{N}(x \\mid m_2, \\sigma_2) ) = \\frac{1}{2} \\Big[\\log\\frac{|\\Sigma_2|}{|\\Sigma_1|} - d + \\text{tr}(\\Sigma_2^{-1}\\Sigma_1) + (\\mu_2 - \\mu_1)^{\\top}\\Sigma_2^{-1}(\\mu_2 - \\mu_1) \\Big] \\, .\n",
    "\\end{aligned}$$\n",
    "\n",
    "For our distribution, this becomes:\n",
    "\n",
    "$$\\int p(y_t|u_t) \\log \\frac{p(y_t|u_t)}{p^\\prime(y_t)} \\, \\mathrm{d}y_t = \\frac{1}{2} \\Big[\\log\\frac{s_*}{|C(AS_{t-1}A^{\\top} + Q) C^{\\top} + R|} - 1 + \\text{tr}(\\frac{(C(AS_{t-1}A^{\\top} + Q) C^{\\top} + R)}{s_*}) + \\frac{\\big(m_* - (C(A m_{t-1} + Bu_t))\\big)^2}{s_*}) \\Big]$$\n",
    "\n",
    "\n",
    "The decomposition is the same for any future state $t$. We may thus unroll the policy as $G(u) = \\sum_{t=k+1}^{TT} G(u_t)$.\n",
    "\n",
    "---\n",
    "\n",
    "The above equations can be written down to form an objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c535a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "function EFE(u::AbstractVector,\n",
    "             params::Vector{Float64},\n",
    "             state::Tuple{Vector{Float64}, Matrix{Float64}}, \n",
    "             goal::Tuple{Float64,Float64}; \n",
    "             s_u::Float64=1.0,\n",
    "             time_horizon::Int64=1)\n",
    "    \n",
    "    # Current dynamics\n",
    "    Ak = [1 Δt; params[1]*Δt params[2]*Δt+1]\n",
    "    Bk = [0, params[3]*Δt]\n",
    "    \n",
    "    # Unpack goal state\n",
    "    m_star, S_star = goal\n",
    "    iS = inv(S_star)\n",
    "    \n",
    "    # Unpack parameters of current state\n",
    "    m_tmin1, S_tmin1 = state\n",
    "    \n",
    "    # Start cumulative sum\n",
    "    cEFE = 0.0\n",
    "    for t in 1:time_horizon\n",
    "        \n",
    "        # State transition p(z_t | u_t)\n",
    "        m_t = Ak*m_tmin1 + Bk*u[t]\n",
    "        S_t = Ak*S_tmin1*Ak'\n",
    "\n",
    "        # Predicted observation \n",
    "        m_pred = dot(C,m_t)\n",
    "        S_pred = dot(C'*S_t,C) + R\n",
    "        \n",
    "        # First term of EFE\n",
    "        ambiguity = 0.5(log(2π) + log(S_pred) - 1)\n",
    "\n",
    "        # Risk\n",
    "        diff_m = (m_pred - m_star)\n",
    "        iS_star = inv(S_star)\n",
    "        risk = 0.5(log(det(S_star)/det(S_pred)) + 1 + diff_m'*iS_star*diff_m + tr(iS_star*S_pred))\n",
    "        \n",
    "        # Prior term\n",
    "        prior_u = 1/(s_u)*u[t]^2\n",
    "        \n",
    "        # Cumulate EFE\n",
    "        cEFE += ambiguity + risk + prior_u\n",
    "        \n",
    "        # Update state recursion\n",
    "        m_tmin1 = m_t\n",
    "        S_tmin1 = S_t\n",
    "                \n",
    "    end\n",
    "    return cEFE\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae93a271",
   "metadata": {},
   "source": [
    "We can use the Optim.jl library to auto-differentiate EFE and minimize it every time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598f0c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_trial = 100\n",
    "time = range(0, step=Δt, length=len_trial)\n",
    "\n",
    "# Time horizon\n",
    "len_horizon = 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14620f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution around goal state\n",
    "goal_state = (y_star, 1e-3)\n",
    "\n",
    "# Limits of controller\n",
    "u_lims = (-100.0, 100.0)\n",
    "\n",
    "# Num samples for particle filter\n",
    "n_samples = 10\n",
    "\n",
    "# Preallocate\n",
    "z_est = (zeros(2,len_trial), zeros(2,2,len_trial))\n",
    "θ_est = (zeros(3,len_trial), zeros(3,3,len_trial))\n",
    "z_sim = zeros(2,len_trial)\n",
    "y_sim = zeros(len_trial)\n",
    "u_sim = zeros(len_trial)\n",
    "\n",
    "# Initial control \n",
    "u_sim[1] = randn()\n",
    "\n",
    "# Initial belief\n",
    "m_0 = zeros(2)\n",
    "S_0 = diagm(ones(2))\n",
    "\n",
    "# Parameter priors\n",
    "μ_0 = θ_true\n",
    "Σ_0 = 1e-4*diagm(ones(3))\n",
    "\n",
    "# Actual initial state\n",
    "z_sim[:,1] = z_0\n",
    "\n",
    "# Start recursion\n",
    "m_kmin1 = m_0\n",
    "S_kmin1 = S_0\n",
    "μ_kmin1 = μ_0\n",
    "Σ_kmin1 = Σ_0\n",
    "\n",
    "sx_ = []\n",
    "policy = zeros(len_horizon)\n",
    "\n",
    "@showprogress for k in 2:len_trial\n",
    "    \n",
    "    \"Interact with environment\"\n",
    "    \n",
    "    # Update system with selected control\n",
    "    y_sim[k], z_sim[:,k] = update(z_sim[:,k-1], u_sim[k-1])\n",
    "   \n",
    "    \"State estimation\"\n",
    "    \n",
    "    # Current dynamics\n",
    "    Ak = [1 Δt; μ_kmin1[1]*Δt μ_kmin1[2]*Δt+1]\n",
    "    Bk = [0, μ_kmin1[3]*Δt]\n",
    "    \n",
    "    # Prediction step\n",
    "    m_k_pred = Ak*m_kmin1 + Bk*u_sim[k-1]\n",
    "    S_k_pred = Ak*S_kmin1*Ak'\n",
    "    \n",
    "    # Update step\n",
    "    K = S_k_pred*(C*inv(dot(C'*S_k_pred,C) + R))\n",
    "    m_k = m_k_pred + K*(y_sim[k] - dot(C,m_k_pred))\n",
    "    S_k = S_k_pred - K*(dot(C'*S_k_pred,C) + R)*K'\n",
    "    \n",
    "    # Store state estimates\n",
    "    z_est[1][:,k] = m_k\n",
    "    z_est[2][:,:,k] = S_k\n",
    "    \n",
    "    \"Parameter estimation\"\n",
    "    \n",
    "    # Sample from previous state\n",
    "    sx_ = rand(MvNormal(m_kmin1, Hermitian(S_kmin1)), n_samples)\n",
    "    # sy_ = rand(MvNormal(m_k,     Hermitian(S_k)),     n_samples)\n",
    "    sy_ = zeros(2,n_samples)\n",
    "    \n",
    "    # Compute weights of samples\n",
    "    # weights = [pdf(MvNormal(m_kmin1, Hermitian(S_kmin1)), sx_[:,i]) for i = 1:n_samples]\n",
    "    \n",
    "    # Push through non-linearity\n",
    "    ss1 = filter(!isnan, filter(isfinite, [g1(sy_[:,i], sx_[:,i], u_sim[k-1], θ2_true, θ3_true) for i = 1:n_samples]))\n",
    "    ss2 = filter(!isnan, filter(isfinite, [g2(sy_[:,i], sx_[:,i], u_sim[k-1], θ1_true, θ3_true) for i = 1:n_samples]))\n",
    "    ss3 = filter(!isnan, filter(isfinite, [g3(sy_[:,i], sx_[:,i], u_sim[k-1], θ1_true, θ2_true) for i = 1:n_samples]))\n",
    "    smm = [mean(ss1), mean(ss2), mean(ss3)]\n",
    "    smS = diagm([var(ss1), var(ss2), var(ss3)])\n",
    "    \n",
    "    # Marginal update as collision of messages\n",
    "    Σ_k = inv(inv(Σ_kmin1) + inv(smS))\n",
    "    μ_k = Σ_k*(inv(Σ_kmin1)*μ_kmin1 + inv(smS)*smm)\n",
    "    \n",
    "    # Store parameter estimates\n",
    "    θ_est[1][:,k] = μ_k\n",
    "    θ_est[2][:,:,k] = Σ_k\n",
    "    \n",
    "    \"Planning\"\n",
    "    \n",
    "    # Single-argument objective\n",
    "    G(u::AbstractVector) = EFE(u, μ_k, (m_k,S_k), goal_state, s_u=1e6, time_horizon=len_horizon)\n",
    "    \n",
    "    # Call minimizer using constrained L-BFGS procedure\n",
    "    results = Optim.optimize(G, u_lims[1], u_lims[2], policy, Fminbox(LBFGS()); autodiff=:forward)\n",
    "    \n",
    "    # Extract minimizing control\n",
    "    policy = Optim.minimizer(results)\n",
    "    u_sim[k] = policy[1] + 1e-12*randn()\n",
    "   \n",
    "    # Update recursion\n",
    "    m_kmin1 = m_k\n",
    "    S_kmin1 = S_k\n",
    "    μ_kmin1 = μ_k\n",
    "    Σ_kmin1 = Σ_k\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454c11de",
   "metadata": {},
   "outputs": [],
   "source": [
    "p301 = plot(time, z_sim[1,:], label=\"true\", ylabel=\"Position\", xlabel=\"Time [s]\")\n",
    "scatter!(time, y_sim, label=\"observations\", color=\"black\")\n",
    "plot!(time, z_est[1][1,:], ribbon=3sqrt.(z_est[2][1,1,:]), label=\"inferred\", color=\"purple\")\n",
    "plot!(time, y_star*ones(len_trial), color=\"green\", alpha=0.5, linestyle=:dash, label=\"setpoint\")\n",
    "p302 = plot(time, u_sim, color=\"red\", ylabel=\"Control\")\n",
    "plot(p301, p302, layout=(2,1), size=(1000,600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eda23e-cf89-43f7-8498-f6a56802911d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p401 = plot(θ_est[1][1,:], ribbon=sqrt.(θ_est[2][1,1,:]))\n",
    "plot!(θ1_true*ones(len_trial), color=\"green\", linestyle=:dash, alpha=0.5, size=(1000,300))\n",
    "\n",
    "p402 = plot(θ_est[1][2,:], ribbon=sqrt.(θ_est[2][2,2,:]))\n",
    "plot!(θ1_true*ones(len_trial), color=\"green\", linestyle=:dash, alpha=0.5, size=(1000,300))\n",
    "\n",
    "p403 = plot(θ_est[1][3,:], ribbon=sqrt.(θ_est[2][3,3,:]))\n",
    "plot!(θ1_true*ones(len_trial), color=\"green\", linestyle=:dash, alpha=0.5, size=(1000,300))\n",
    "\n",
    "plot(p401,p402,p403, layout=(3,1), size=(1000,600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5384ffc6-9121-4ad2-a29c-97ee79682599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0adbc45b",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "Let's visualize the planned trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13988c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_trial = 100\n",
    "time = range(0, step=Δt, length=len_trial)\n",
    "\n",
    "# Time horizon\n",
    "len_horizon = 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa758a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "function planned_trajectory(policy, current_state, params)\n",
    "    \"Generate future states and observations\"\n",
    "    \n",
    "    # Matrices\n",
    "    Ak = [1 Δt; params[1]*Δt params[2]*Δt+1]\n",
    "    Bk = [0, params[3]*Δt]\n",
    "    \n",
    "    # Extract time horizon\n",
    "    time_horizon = length(policy)\n",
    "    \n",
    "    # Unpack parameters of current state\n",
    "    m_tmin1, S_tmin1 = current_state\n",
    "    \n",
    "    # Track predicted observations\n",
    "    z_m = zeros(2,time_horizon)\n",
    "    z_S = zeros(2,2,time_horizon)\n",
    "    y_m = zeros(time_horizon)\n",
    "    y_v = zeros(time_horizon)\n",
    "    \n",
    "    for t in 1:time_horizon\n",
    "        \n",
    "        # State transition\n",
    "        z_m[:,t] = Ak*m_tmin1 + Bk*policy[t]\n",
    "        z_S[:,:,t] = Ak*S_tmin1*Ak'\n",
    "        \n",
    "        # Predicted observation \n",
    "        y_m[t] = dot(C,z_m[:,t])\n",
    "        y_v[t] = dot(C'*z_S[:,:,t],C) + R\n",
    "        \n",
    "        # Update previous state\n",
    "        m_tmin1 = z_m[:,t]\n",
    "        S_tmin1 = z_S[:,:,t]\n",
    "        \n",
    "    end\n",
    "    return z_m, z_S, y_m, y_v\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f3bd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution around goal state\n",
    "goal_state = (y_star, 1e-3)\n",
    "\n",
    "# Limits of controller\n",
    "u_lims = (-100.0, 100.0)\n",
    "\n",
    "# Num samples for particle filter\n",
    "n_samples = 100\n",
    "\n",
    "# Preallocate\n",
    "θ_est = (zeros(len_trial), zeros(len_trial))\n",
    "z_est = (zeros(2,len_trial), zeros(2,2,len_trial))\n",
    "z_pln = (zeros(len_trial, 2,len_horizon), zeros(len_trial, 2,2,len_horizon))\n",
    "y_pln = (zeros(len_trial, len_horizon), zeros(len_trial,len_horizon))\n",
    "z_sim = zeros(2,len_trial)\n",
    "y_sim = zeros(len_trial)\n",
    "u_sim = zeros(len_trial)\n",
    "\n",
    "# Initial belief\n",
    "m_0 = zeros(2)\n",
    "S_0 = diagm(ones(2))\n",
    "\n",
    "# Parameter priors\n",
    "μ_0 = 0.0\n",
    "Σ_0 = 1.0\n",
    "\n",
    "# Actual initial state\n",
    "z_sim[:,1] = z_0\n",
    "\n",
    "# Start recursion\n",
    "m_kmin1 = m_0\n",
    "S_kmin1 = S_0\n",
    "μ_kmin1 = μ_0\n",
    "Σ_kmin1 = Σ_0\n",
    "\n",
    "@showprogress for k in 2:len_trial\n",
    "    \n",
    "    \"Interact with environment\"\n",
    "    \n",
    "    # Update system with selected control\n",
    "    y_sim[k], z_sim[:,k] = update(z_sim[:,k-1], u_sim[k-1])\n",
    "   \n",
    "    \"State estimation\"\n",
    "    \n",
    "    # Update matrices\n",
    "    A = [1 Δt; μ_kmin1*Δt θ2_true*Δt+1]\n",
    "    B = [0, θ3_true*Δt]\n",
    "    \n",
    "    # Prediction step\n",
    "    m_k_pred = A*m_kmin1 + B*u_sim[k-1]\n",
    "    S_k_pred = A*S_kmin1*A'\n",
    "    \n",
    "    # Update step\n",
    "    K = S_k_pred*(C*inv(dot(C'*S_k_pred,C) + R))\n",
    "    m_k = m_k_pred + K*(y_sim[k] - dot(C,m_k_pred))\n",
    "    S_k = S_k_pred - K*(dot(C'*S_k_pred,C) + R)*K'\n",
    "    \n",
    "    # Store state estimates\n",
    "    z_est[1][:,k] = m_k\n",
    "    z_est[2][:,:,k] = S_k\n",
    "    \n",
    "    \"Parameter estimation\"\n",
    "    \n",
    "    # Sample from previous state\n",
    "    sx_ = rand(MvNormal(m_kmin1, Hermitian(S_kmin1)), n_samples)\n",
    "    # sy_ = rand(MvNormal(m_k,     S_k),     n_samples)\n",
    "    sy_ = zeros(2,n_samples)\n",
    "    \n",
    "    ss = [g1(sy_[:,i], sx_[:,i], u_sim[k-1], θ2_true, θ3_true) for i = 1:n_samples]\n",
    "    smm = mean(ss)\n",
    "    smS = var(ss)\n",
    "    \n",
    "    # Marginal update as collision of messages\n",
    "    Σ_k = inv(inv(Σ_kmin1) + inv(smS))\n",
    "    μ_k = Σ_k*(inv(Σ_kmin1)*μ_kmin1 + inv(smS)*smm)\n",
    "    \n",
    "    # Store parameter estimates\n",
    "    θ_est[1][k] = μ_k\n",
    "    θ_est[2][k] = Σ_k\n",
    "    \n",
    "    \"Planning\"\n",
    "    \n",
    "    params = [μ_kmin1, θ2_true, θ3_true]\n",
    "    \n",
    "    # Single-argument objective\n",
    "    G(u::AbstractVector) = EFE(u, params, (m_k,S_k), goal_state, s_u=1e6, time_horizon=len_horizon)\n",
    "    \n",
    "    # Call minimizer using constrained L-BFGS procedure\n",
    "    results = Optim.optimize(G, u_lims[1], u_lims[2], zeros(len_horizon), Fminbox(LBFGS()); autodiff=:forward)\n",
    "    policy = Optim.minimizer(results)\n",
    "    \n",
    "    # Planning\n",
    "    z_pln[1][k,:,:], z_pln[2][k,:,:,:], y_pln[1][k,:], y_pln[2][k,:] = planned_trajectory(policy, (m_k,S_k), params)\n",
    "    \n",
    "    # Execute first planned action only\n",
    "    u_sim[k] = policy[1]\n",
    "   \n",
    "    # Update recursion\n",
    "    m_kmin1 = m_k\n",
    "    S_kmin1 = S_k\n",
    "    μ_kmin1 = μ_k\n",
    "    Σ_kmin1 = Σ_k\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326b1f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = @animate for k = 2:(len_trial-len_horizon)\n",
    "    \n",
    "    p201 = plot(time, y_star*ones(len_trial), color=\"green\", linestyle=:dash, label=\"setpoint\", ylims=[-1.5, 0.0])\n",
    "    scatter!(time[1:k], y_sim[1:k], color=\"black\", label=\"observations\")\n",
    "    plot!(time[k:k+len_horizon-1], y_pln[1][k,:], ribbon=sqrt.(y_pln[2][k,:]), label=\"planned\", color=\"orange\")\n",
    "    \n",
    "    p202 = plot(time[1:k], z_sim[1,1:k], xlims=[time[1], time[end]], label=\"true\", ylabel=\"Position\", xlabel=\"Time [s]\")\n",
    "    plot!(time[k:k+len_horizon-1], z_pln[1][k,1,:], ribbon=sqrt.(z_pln[2][k,1,1,:]), label=\"planned\", color=\"orange\")\n",
    "    plot!(time[1:k], z_est[1][1,1:k], ribbon=sqrt.(z_est[2][1,1,1:k]), label=\"inferred\", color=\"purple\", ylims=[-1,0])\n",
    "    \n",
    "    p203 = plot(time[1:k], u_sim[1:k], xlims=[time[1], time[end]], color=\"red\", ylabel=\"Control\", ylims=u_lims.*1.1)\n",
    "    \n",
    "    plot(p201, p202, p203, layout=(3,1), size=(900,900))\n",
    "end\n",
    "gif(anim, \"figures/planning.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff21a0d8-1286-4b0c-b026-511a25afe1a3",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74da5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "function EFE_analysis(u::AbstractVector, \n",
    "                      params::Vector{Float64},\n",
    "                      state::Tuple{Vector{Float64}, Matrix{Float64}}, \n",
    "                      goal::Tuple{Float64,Float64};\n",
    "                      Q::AbstractMatrix=zeros(2,2),\n",
    "                      time_horizon::Int64=1)\n",
    "    \n",
    "    # Matrices\n",
    "    A = [1 Δt; params[1]*Δt params[2]*Δt+1]\n",
    "    B = [0, params[3]*Δt]\n",
    "    \n",
    "    # Unpack goal state\n",
    "    m_star, S_star = goal\n",
    "    iS = inv(S_star)\n",
    "    \n",
    "    # Unpack parameters of current state\n",
    "    m_tmin1, S_tmin1 = state\n",
    "    \n",
    "    # Start cumulative sum\n",
    "    cAmbiguity = Vector(undef, time_horizon)\n",
    "    cRisk      = Vector(undef, time_horizon)\n",
    "    for t in 1:time_horizon\n",
    "        \n",
    "        # State transition p(z_t | u_t)\n",
    "        m_t = A*m_tmin1 + B*u[t]\n",
    "        S_t = A*S_tmin1*A'\n",
    "\n",
    "        # Predicted observation \n",
    "        m_pred = dot(C,m_t)\n",
    "        S_pred = dot(C'*S_t,C) + R\n",
    "        \n",
    "        # First term of EFE\n",
    "        ambiguity = 0.5(log(2π) + log(S_pred) - 1)\n",
    "\n",
    "        # D_KL[p(y_t | u_t) || p'(y_t)]\n",
    "        risk = 0.5(log(det(S_star)/det(S_pred)) + 1 + (m_pred-m_star)'*iS*(m_pred-m_star) + tr(iS*S_pred))\n",
    "        \n",
    "        # Cumulate EFE\n",
    "        cAmbiguity[t] = ambiguity\n",
    "        cRisk[t]      = risk\n",
    "        \n",
    "        # Update state recursion\n",
    "        m_tmin1 = m_t\n",
    "        S_tmin1 = S_t\n",
    "                \n",
    "    end\n",
    "    return cAmbiguity, cRisk\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4674b105-2475-4f01-bc5d-d4754030f531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution around goal state\n",
    "goal_state = (y_star, 1e-3)\n",
    "\n",
    "# Limits of controller\n",
    "u_lims = (-100.0, 100.0)\n",
    "\n",
    "# Num samples for particle filter\n",
    "n_samples = 1000\n",
    "\n",
    "# Preallocate\n",
    "θ_est = (zeros(len_trial), zeros(len_trial))\n",
    "z_est = (zeros(2,len_trial), zeros(2,2,len_trial))\n",
    "z_pln = (zeros(len_trial, 2,len_horizon), zeros(len_trial, 2,2,len_horizon))\n",
    "y_pln = (zeros(len_trial, len_horizon), zeros(len_trial,len_horizon))\n",
    "z_sim = zeros(2,len_trial)\n",
    "y_sim = zeros(len_trial)\n",
    "u_sim = zeros(len_trial)\n",
    "cA    = zeros(len_trial, len_horizon)\n",
    "cR    = zeros(len_trial, len_horizon)\n",
    "\n",
    "# Initial belief\n",
    "m_0 = zeros(2)\n",
    "S_0 = diagm(ones(2))\n",
    "\n",
    "# Parameter priors\n",
    "μ_0 = 0.0\n",
    "Σ_0 = 1.0\n",
    "\n",
    "# Actual initial state\n",
    "z_sim[:,1] = z_0\n",
    "\n",
    "# Start recursion\n",
    "m_kmin1 = m_0\n",
    "S_kmin1 = S_0\n",
    "μ_kmin1 = μ_0\n",
    "Σ_kmin1 = Σ_0\n",
    "\n",
    "@showprogress for k in 2:len_trial\n",
    "    \n",
    "    \"Interact with environment\"\n",
    "    \n",
    "    # Update system with selected control\n",
    "    y_sim[k], z_sim[:,k] = update(z_sim[:,k-1], u_sim[k-1])\n",
    "   \n",
    "    \"State estimation\"\n",
    "    \n",
    "    # Update matrices\n",
    "    A = [1 Δt; μ_kmin1*Δt θ2_true*Δt+1]\n",
    "    B = [0, θ3_true*Δt]\n",
    "    \n",
    "    # Prediction step\n",
    "    m_k_pred = A*m_kmin1 + B*u_sim[k-1]\n",
    "    S_k_pred = A*S_kmin1*A' + 1e-6*diagm(ones(2))\n",
    "    \n",
    "    # Update step\n",
    "    K = S_k_pred*(C*inv(dot(C'*S_k_pred,C) + R))\n",
    "    m_k = m_k_pred + K*(y_sim[k] - dot(C,m_k_pred))\n",
    "    S_k = S_k_pred - K*(dot(C'*S_k_pred,C) + R)*K'\n",
    "    \n",
    "    # Store state estimates\n",
    "    z_est[1][:,k] = m_k\n",
    "    z_est[2][:,:,k] = S_k\n",
    "    \n",
    "    \"Parameter estimation\"\n",
    "    \n",
    "    # Sample from previous state\n",
    "    sx_ = rand(MvNormal(m_kmin1, Hermitian(S_kmin1)), n_samples)\n",
    "    # sy_ = rand(MvNormal(m_k,     S_k),     n_samples)\n",
    "    sy_ = zeros(2,n_samples)\n",
    "    \n",
    "    ss = [g1(sy_[:,i], sx_[:,i], u_sim[k-1], θ2_true, θ3_true) for i = 1:n_samples]\n",
    "    smm = mean(ss)\n",
    "    smS = var(ss)\n",
    "    \n",
    "    # Marginal update as collision of messages\n",
    "    Σ_k = inv(inv(Σ_kmin1) + inv(smS))\n",
    "    μ_k = Σ_k*(inv(Σ_kmin1)*μ_kmin1 + inv(smS)*smm)\n",
    "    \n",
    "    # Store parameter estimates\n",
    "    θ_est[1][k] = μ_k\n",
    "    θ_est[2][k] = Σ_k\n",
    "    \n",
    "    \"Planning\"\n",
    "    \n",
    "    params = [μ_kmin1, θ2_true, θ3_true]\n",
    "    \n",
    "    # Single-argument objective\n",
    "    G(u::AbstractVector) = EFE(u, params, (m_k,S_k), goal_state, s_u=1e6, time_horizon=len_horizon)\n",
    "    \n",
    "    # Call minimizer using constrained L-BFGS procedure\n",
    "    results = Optim.optimize(G, u_lims[1], u_lims[2], zeros(len_horizon), Fminbox(LBFGS()); autodiff=:forward)\n",
    "    policy = Optim.minimizer(results)\n",
    "    \n",
    "    # Planning\n",
    "    z_pln[1][k,:,:], z_pln[2][k,:,:,:], y_pln[1][k,:], y_pln[2][k,:] = planned_trajectory(policy, (m_k,S_k), params)\n",
    "    cA[k,:], cR[k,:] = EFE_analysis(policy, params, (m_k, S_k), goal_state, time_horizon=len_horizon)\n",
    "    \n",
    "    # Execute first planned action only\n",
    "    u_sim[k] = policy[1]\n",
    "   \n",
    "    # Update recursion\n",
    "    m_kmin1 = m_k\n",
    "    S_kmin1 = S_k\n",
    "    μ_kmin1 = μ_k\n",
    "    Σ_kmin1 = Σ_k\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656fd1e2-e1f0-4601-8bf4-5a8aeb28fc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = @animate for k = 2:(len_trial-len_horizon)\n",
    "    \n",
    "    p201 = plot(time, y_star*ones(len_trial), color=\"green\", linestyle=:dash, label=\"setpoint\", ylims=[-1.5, 0.0])\n",
    "    scatter!(time[1:k], y_sim[1:k], color=\"black\", label=\"observations\")\n",
    "    plot!(time[k:k+len_horizon-1], y_pln[1][k,:], ribbon=sqrt.(y_pln[2][k,:]), label=\"planned\", color=\"orange\")\n",
    "    \n",
    "    p202 = plot(time[1:k], z_sim[1,1:k], xlims=[time[1], time[end]], label=\"true\", ylabel=\"Position\", xlabel=\"Time [s]\")\n",
    "    plot!(time[k:k+len_horizon-1], z_pln[1][k,1,:], ribbon=sqrt.(z_pln[2][k,1,1,:]), label=\"planned\", color=\"orange\")\n",
    "    plot!(time[1:k], z_est[1][1,1:k], ribbon=sqrt.(z_est[2][1,1,1:k]), label=\"inferred\", color=\"purple\", ylims=[-1,0])\n",
    "    \n",
    "    p203 = plot(time[1:k], u_sim[1:k], xlims=[time[1], time[end]], color=\"red\", ylabel=\"Control\", ylims=u_lims.*1.1)\n",
    "    \n",
    "    plot(p201, p202, p203, layout=(3,1), size=(900,900))\n",
    "end\n",
    "gif(anim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf14c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "tix = 2:len_trial\n",
    "plot(time[tix], cA[tix,end], label=\"ambiguity\", size=(1000,200))\n",
    "plot!(time[tix], cR[tix,end], label=\"risk\", linewidth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6457eb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(θ_est[1], ribbon=sqrt.(θ_est[2]))\n",
    "plot!(θ1_true*ones(len_trial), color=\"green\", linestyle=:dash, alpha=0.5, size=(1000,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5120158f-5bb5-48cf-a958-c2f2efd19f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "julia 1.8.3",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
